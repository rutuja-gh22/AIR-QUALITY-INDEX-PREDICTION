# -*- coding: utf-8 -*-
"""ML_mini_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sSBzsxjj8DrVONGtfMDCbhRTwSbJXCfC

**AIR QUALITY INDEX PREDICTION**

BY:- UCE2022469, UCE2022468

**PROBLEM STATEMENT** :- Given dataset, calculate the responses of a gas multisensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer.

**INTRODUCTION** :- The dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level,within an Italian city. Data were recorded from March 2004 to February 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses.Missing values are tagged with -200 value. Hence, responses of the gas multisensor device is calculated through **Linear Regression, KNN classification, Time Series Analysis**. We aim to provide users with an acknowledgment of the indexes associated with all pollutants generated by gases, enabling them to take necessary actions accordingly.

**DATASET INFORMATION** :-

https://drive.google.com/file/d/1iH8waMfK3x8SRIY4WAvGmEhUa4ZcKbWz/view?usp=drive_link
"""

import numpy as np
import pandas as pd

# loading the dataset
aq_data = pd.read_csv("/content/AirQualityUCI.csv")

aq_data.head()

# printing last 5 rows
aq_data.tail()

aq_data.shape

# getting some info about the data
aq_data.info()

aq_data.isnull().sum()

# Fill with mean
mean_AH = aq_data['AH'].mean()
aq_data['AH'] = aq_data['AH'].fillna(mean_AH)

# Fill with median
median_AH = aq_data['AH'].median()
aq_data['AH'] = aq_data['AH'].fillna(median_AH)

# Fill with a constant (e.g., 0)
aq_data['AH'] = aq_data['AH'].fillna(0)

aq_data.isnull().sum()

aq_data.isin([-200]).sum(axis=0)

aq_data.tail()

aq_data = aq_data.replace(to_replace= -200, value = np.NaN)

aq_data.isnull().sum()

aq_data.tail()

# Calculate the mean of the 'NMHC(GT)' column
mean_NMHC = aq_data['NMHC(GT)'].mean()

# Replace NaN values in the 'NMHC(GT)' column with the mean
aq_data['NMHC(GT)'] = aq_data['NMHC(GT)'].fillna(mean_NMHC)

aq_data.tail()

# Calculate the mean of the 'CO(GT)' column (excluding NaN values)
mean_CO_GT = aq_data['CO(GT)'].mean()

# Replace NaN values in the 'CO(GT)' column with the mean
aq_data['CO(GT)'] = aq_data['CO(GT)'].fillna(mean_CO_GT)

aq_data.tail()

aq_data.head()

aq_data.isnull().sum()

# Define the list of columns to handle missing values for
columns_to_impute = ['PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
                     'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)',
                     'T', 'RH', 'AH']

# Iterate over each column and handle missing values
for column in columns_to_impute:
    # Calculate the mean of the column
    mean_value = aq_data[column].mean()
    # Replace NaN values in the column with the mean
    aq_data[column] = aq_data[column].fillna(mean_value)

aq_data.isnull().sum()

"""**DONE WITH DATA PROCESSING**

IN TIME SERIES ANALYSIS, one column should be date and time info and other one as the desired gas
"""

date_info = pd.to_datetime(aq_data['Date'], format='%d/%m/%Y')
print(date_info)

time_info = aq_data['Time']
print(time_info)

time_info = time_info.apply(lambda x : x.replace('.', ':'))

print(type(date_info))
print(type(time_info))

date_time = pd.concat([date_info, time_info], axis = 1)

date_time['ds'] = date_time['Date'].astype(str)+' '+date_time['Time'].astype(str)

date_time.head()

date_time.info()

data = pd.DataFrame()

# Check if the 'ds' column contains the value "-200"
has_negative_200 = date_time['ds'].str.contains("-200")

# Count the number of occurrences of "-200"
count_negative_200 = has_negative_200.sum()

# Display the count and the rows where "-200" is present
print("Number of occurrences of '-200':", count_negative_200)
print("Rows where '-200' is present:")
print(date_time[has_negative_200])

# Filter out the row where '-200' is present in the 'ds' column
date_time = date_time[~has_negative_200]

# Reset the index after removing the row
date_time.reset_index(drop=True, inplace=True)

# Verify that '-200' is removed
print(date_time)

data['ds'] = pd.to_datetime(date_time['ds'])

data['y'] = aq_data['RH']
data.head()

pip install prophet

from prophet import Prophet
m = Prophet()
m.fit(data)

future = m.make_future_dataframe(periods=365, freq='H')
future.tail()

forecast = m.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1 = m.plot(forecast)

fig2 = m.plot_components(forecast)

"""**TIME SERIES ANALYSIS DONE**"""

import matplotlib.pyplot as plt
aq_data.hist(figsize = (20,20))
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Define a custom pastel color palette
pastel_palette = sns.color_palette("pastel")

# Set the context and style
sns.set_context("talk")
sns.set_style("whitegrid")

# Plotting the bar plot with pastel colors
plt.figure(figsize=(20,6))
ax = sns.barplot(x='Time', y='NOx(GT)', data=aq_data, ci=False, palette=pastel_palette)
plt.xlabel('Hours')
plt.ylabel('Total Nitrogen Oxides (NOx) in ppb') # Parts per billion (ppb)
plt.title("Mean Total Nitrogen Oxides (NOx) Frequency During Days")

# Rotate x-axis labels for better readability
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

# Show plot
plt.show()

X = aq_data.drop(['AH','Time'], axis=1)

y= aq_data['AH']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.preprocessing import RobustScaler
# Assuming x_train and x_test are DataFrames containing your training and testing data
# Drop non-numeric columns, such as date columns
x_train_numeric = x_train.select_dtypes(include=[np.number])
x_test_numeric = x_test.select_dtypes(include=[np.number])

# Scale the numeric features using RobustScaler
scaler = RobustScaler()
x_train_scaled = scaler.fit_transform(x_train_numeric)
x_test_scaled = scaler.transform(x_test_numeric)

from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(x_train_scaled, y_train)

print(lm.intercept_)
prediction = lm.predict(x_test_scaled)
plt.scatter(y_test, prediction, c="blue", alpha=0.3)
plt.xlabel('Measured')
plt.ylabel('Predicted')
plt.title('Linear Regression Predicted vs Actual')

import seaborn as sns

# Scatter plot of predicted vs. actual values
sns.scatterplot(x=y_test, y=prediction, color='blue', alpha=0.3)

# Add a regression line
sns.regplot(x=y_test, y=prediction, scatter=False, color='red')

# Set labels and title
plt.xlabel('Measured')
plt.ylabel('Predicted')
plt.title('Linear Regression Predicted vs Actual')

# Show plot
plt.show()

score_train = lm.score(x_train_scaled, y_train)
print(score_train)

score_test = lm.score(x_test_scaled, y_test)
score_test

from sklearn import metrics
print('MAE:',metrics.mean_absolute_error(y_test, prediction))
print('MSE:',metrics.mean_squared_error(y_test, prediction))
print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, prediction)))

"""**REGRESSION DONE**"""

X1 = aq_data.drop(['AH','Time'], axis=1)
y1 = aq_data['AH']
from sklearn.model_selection import train_test_split
x_train1, x_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=0)
from sklearn.preprocessing import RobustScaler
# Assuming x_train and x_test are DataFrames containing your training and testing data
# Drop non-numeric columns, such as date columns
x_train_numeric1 = x_train1.select_dtypes(include=[np.number])
x_test_numeric1 = x_test1.select_dtypes(include=[np.number])
# Scale the numeric features using RobustScaler
scaler1 = RobustScaler()
x_train_scaled1 = scaler1.fit_transform(x_train_numeric1)
x_test_scaled1 = scaler1.transform(x_test_numeric1)

from sklearn.neighbors import KNeighborsRegressor
knn=KNeighborsRegressor(n_neighbors=5)
knn.fit(x_train_scaled1,y_train1)
prediction = knn.predict(x_test_scaled1)

from sklearn.metrics import confusion_matrix
# Calculate the mean of the target variable
threshold_value = y_train1.mean()

# Convert the regression problem into a classification one using the mean value as the threshold
y_test_class = (y_test1 > threshold_value).astype(int)
prediction_class = (prediction > threshold_value).astype(int)

# Calculate confusion matrix
cm = confusion_matrix(y_test_class, prediction_class)

print("Confusion Matrix:")
print(cm)

knn_train = knn.score(x_train_scaled1,y_train1)
print(knn_train)

kreg_test = knn.score(x_test_scaled1,y_test1)
print(kreg_test)

"""**KNN DONE**

**CONCLUSION** :- After conducting a comprehensive analysis utilizing various regression techniques, including linear regression, and k-nearest neighbors (KNN), Forecasting we have gained valuable insights into the factors influencing pollutant levels. Through our exploration, we identified significant predictors and their impacts on pollution levels, aiding in the development of effective mitigation strategies. By leveraging regression analysis, we can now make informed predictions and recommendations to mitigate pollution, thereby contributing to environmental sustainability and public health.

**REFERENCES** :-

https://www.kaggle.com/code/parimalbhoyar25/air-quality-uci

FB PROPHET DOCUMENTATION: https://facebook.github.io/prophet/docs/quick_start.html
"""